{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "from microservice.models.model_b import SimpleNN\n",
    "from models.utils import load_default_data, calculate_expenses\n",
    "from microservice.features import extract_users_data, prepare_data_for_predict\n",
    "from sklearn.metrics import roc_auc_score as roc\n",
    "from microservice.models.model_b import NeuralNetworkRegressor\n",
    "from models.train_utils_nn import train\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [],
   "source": [
    "seed = 213769420\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "data": {
      "text/plain": "     user_id  expenses\n0        102      0.00\n1        103    554.77\n2        104   2332.01\n3        105      0.00\n4        106      0.00\n..       ...       ...\n195      297    109.00\n196      298   2399.00\n197      299      0.00\n198      300      0.00\n199      301      0.00\n\n[200 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>expenses</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>102</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>103</td>\n      <td>554.77</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>104</td>\n      <td>2332.01</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>105</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>106</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>297</td>\n      <td>109.00</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>298</td>\n      <td>2399.00</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>299</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>300</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>301</td>\n      <td>0.00</td>\n    </tr>\n  </tbody>\n</table>\n<p>200 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sessions, test, products, users, deliveries = load_default_data()\n",
    "targets = calculate_expenses(test, products, users)\n",
    "targets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "# targets['expenses'] = targets['expenses'].apply(lambda b: 1 if b > 5000 else 0) binarization\n",
    "targets['expenses'] = targets['expenses'].apply(lambda x: x/100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "data": {
      "text/plain": "     user_id  expenses\n0        102    0.0000\n1        103    5.5477\n2        104   23.3201\n3        105    0.0000\n4        106    0.0000\n..       ...       ...\n195      297    1.0900\n196      298   23.9900\n197      299    0.0000\n198      300    0.0000\n199      301    0.0000\n\n[200 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>expenses</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>102</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>103</td>\n      <td>5.5477</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>104</td>\n      <td>23.3201</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>105</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>106</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>297</td>\n      <td>1.0900</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>298</td>\n      <td>23.9900</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>299</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>300</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>301</td>\n      <td>0.0000</td>\n    </tr>\n  </tbody>\n</table>\n<p>200 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "data": {
      "text/plain": "     user_id  expenses  products_bought  events_number  sessions_number  \\\n0        139  23400.85               49            255               66   \n1        242  44518.18               58            327               86   \n2        108  16312.03               23            129               32   \n3        143   7214.08               15             84               26   \n4        140  32333.55               43            306               83   \n..       ...       ...              ...            ...              ...   \n195      289      0.00                0              7                1   \n196      225    109.00                1              2                1   \n197      152      0.00                0              5                2   \n198      162     78.96                2              6                2   \n199      229    245.00                1              5                1   \n\n     average_discount  average_discount_on_bought      city  \n0           11.019608                   10.816327    Kraków  \n1           11.223242                   10.603448     Radom  \n2            9.418605                    9.347826     Radom  \n3           11.250000                   11.666667    Kraków  \n4           10.130719                    9.651163    Poznań  \n..                ...                         ...       ...  \n195         20.000000                    0.000000  Warszawa  \n196         10.000000                   10.000000  Warszawa  \n197          2.000000                    0.000000    Poznań  \n198         13.333333                   12.500000  Szczecin  \n199         10.000000                   10.000000     Radom  \n\n[200 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>expenses</th>\n      <th>products_bought</th>\n      <th>events_number</th>\n      <th>sessions_number</th>\n      <th>average_discount</th>\n      <th>average_discount_on_bought</th>\n      <th>city</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>139</td>\n      <td>23400.85</td>\n      <td>49</td>\n      <td>255</td>\n      <td>66</td>\n      <td>11.019608</td>\n      <td>10.816327</td>\n      <td>Kraków</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>242</td>\n      <td>44518.18</td>\n      <td>58</td>\n      <td>327</td>\n      <td>86</td>\n      <td>11.223242</td>\n      <td>10.603448</td>\n      <td>Radom</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>108</td>\n      <td>16312.03</td>\n      <td>23</td>\n      <td>129</td>\n      <td>32</td>\n      <td>9.418605</td>\n      <td>9.347826</td>\n      <td>Radom</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>143</td>\n      <td>7214.08</td>\n      <td>15</td>\n      <td>84</td>\n      <td>26</td>\n      <td>11.250000</td>\n      <td>11.666667</td>\n      <td>Kraków</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>140</td>\n      <td>32333.55</td>\n      <td>43</td>\n      <td>306</td>\n      <td>83</td>\n      <td>10.130719</td>\n      <td>9.651163</td>\n      <td>Poznań</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>289</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>7</td>\n      <td>1</td>\n      <td>20.000000</td>\n      <td>0.000000</td>\n      <td>Warszawa</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>225</td>\n      <td>109.00</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>Warszawa</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>152</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>Poznań</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>162</td>\n      <td>78.96</td>\n      <td>2</td>\n      <td>6</td>\n      <td>2</td>\n      <td>13.333333</td>\n      <td>12.500000</td>\n      <td>Szczecin</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>229</td>\n      <td>245.00</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>Radom</td>\n    </tr>\n  </tbody>\n</table>\n<p>200 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_net = NeuralNetworkRegressor()\n",
    "users_data = extract_users_data(train_sessions, users, products)\n",
    "users_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss 1.73e+02\n",
      "Epoch 1 loss 2.84e+02\n",
      "Epoch 2 loss 1.87e+02\n",
      "Epoch 3 loss 2.16e+02\n",
      "Epoch 4 loss 1.85e+02\n",
      "Epoch 5 loss 2.16e+02\n",
      "Epoch 6 loss 1.92e+02\n",
      "Epoch 7 loss 1.9e+02\n",
      "Epoch 8 loss 1.81e+02\n",
      "Epoch 9 loss 2.47e+02\n",
      "Epoch 10 loss 2.23e+02\n",
      "Epoch 11 loss 2.66e+02\n",
      "Epoch 12 loss 2.25e+02\n",
      "Epoch 13 loss 2.03e+02\n",
      "Epoch 14 loss 1.9e+02\n",
      "Epoch 15 loss 1.94e+02\n",
      "Epoch 16 loss 1.77e+02\n",
      "Epoch 17 loss 2.07e+02\n",
      "Epoch 18 loss 2.86e+02\n",
      "Epoch 19 loss 2.17e+02\n",
      "Epoch 20 loss 1.89e+02\n",
      "Epoch 21 loss 2.28e+02\n",
      "Epoch 22 loss 2.16e+02\n",
      "Epoch 23 loss 2.54e+02\n",
      "Epoch 24 loss 2.06e+02\n",
      "Epoch 25 loss 2.7e+02\n",
      "Epoch 26 loss 1.76e+02\n",
      "Epoch 27 loss 2.57e+02\n",
      "Epoch 28 loss 2.73e+02\n",
      "Epoch 29 loss 2.68e+02\n",
      "Epoch 30 loss 1.81e+02\n",
      "Epoch 31 loss 1.89e+02\n",
      "Epoch 32 loss 2.15e+02\n",
      "Epoch 33 loss 2.17e+02\n",
      "Epoch 34 loss 1.98e+02\n",
      "Epoch 35 loss 1.78e+02\n",
      "Epoch 36 loss 2.97e+02\n",
      "Epoch 37 loss 1.75e+02\n",
      "Epoch 38 loss 1.83e+02\n",
      "Epoch 39 loss 2.67e+02\n",
      "Epoch 40 loss 1.8e+02\n",
      "Epoch 41 loss 1.94e+02\n",
      "Epoch 42 loss 1.81e+02\n",
      "Epoch 43 loss 2.57e+02\n",
      "Epoch 44 loss 2.45e+02\n",
      "Epoch 45 loss 1.94e+02\n",
      "Epoch 46 loss 2.98e+02\n",
      "Epoch 47 loss 1.96e+02\n",
      "Epoch 48 loss 3.15e+02\n",
      "Epoch 49 loss 1.66e+02\n",
      "Epoch 50 loss 1.93e+02\n",
      "Epoch 51 loss 1.81e+02\n",
      "Epoch 52 loss 2.15e+02\n",
      "Epoch 53 loss 1.92e+02\n",
      "Epoch 54 loss 2.15e+02\n",
      "Epoch 55 loss 2.32e+02\n",
      "Epoch 56 loss 2.19e+02\n",
      "Epoch 57 loss 1.97e+02\n",
      "Epoch 58 loss 1.95e+02\n",
      "Epoch 59 loss 1.79e+02\n",
      "Epoch 60 loss 1.96e+02\n",
      "Epoch 61 loss 2.12e+02\n",
      "Epoch 62 loss 2.05e+02\n",
      "Epoch 63 loss 2.43e+02\n",
      "Epoch 64 loss 1.83e+02\n",
      "Epoch 65 loss 1.77e+02\n",
      "Epoch 66 loss 2.18e+02\n",
      "Epoch 67 loss 1.98e+02\n",
      "Epoch 68 loss 1.9e+02\n",
      "Epoch 69 loss 1.96e+02\n",
      "Epoch 70 loss 2.01e+02\n",
      "Epoch 71 loss 2.16e+02\n",
      "Epoch 72 loss 1.93e+02\n",
      "Epoch 73 loss 1.91e+02\n",
      "Epoch 74 loss 2.18e+02\n",
      "Epoch 75 loss 1.7e+02\n",
      "Epoch 76 loss 2.36e+02\n",
      "Epoch 77 loss 1.8e+02\n",
      "Epoch 78 loss 2.23e+02\n",
      "Epoch 79 loss 1.8e+02\n",
      "Epoch 80 loss 3.32e+02\n",
      "Epoch 81 loss 2.05e+02\n",
      "Epoch 82 loss 2.15e+02\n",
      "Epoch 83 loss 2.3e+02\n",
      "Epoch 84 loss 1.79e+02\n",
      "Epoch 85 loss 2.21e+02\n",
      "Epoch 86 loss 1.82e+02\n",
      "Epoch 87 loss 1.95e+02\n",
      "Epoch 88 loss 1.76e+02\n",
      "Epoch 89 loss 2.53e+02\n",
      "Epoch 90 loss 3.15e+02\n",
      "Epoch 91 loss 1.67e+02\n",
      "Epoch 92 loss 2.41e+02\n",
      "Epoch 93 loss 1.66e+02\n",
      "Epoch 94 loss 2.63e+02\n",
      "Epoch 95 loss 2.51e+02\n",
      "Epoch 96 loss 1.96e+02\n",
      "Epoch 97 loss 2e+02\n",
      "Epoch 98 loss 1.98e+02\n",
      "Epoch 99 loss 1.88e+02\n"
     ]
    }
   ],
   "source": [
    "train(some_net, users_data, targets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "data": {
      "text/plain": "array([2.5522861 , 6.670182  , 1.1285622 , 0.99783105, 4.2777514 ,\n       2.145917  , 4.229818  , 2.3139265 , 0.45462483, 1.608789  ,\n       2.7811337 , 0.5814664 , 0.43894744, 2.1252663 , 3.6308777 ,\n       1.0397326 , 1.3970548 , 2.6281579 , 0.41820455, 2.0558407 ,\n       1.8769951 , 2.0723605 , 1.0095036 , 1.3822054 , 0.9210697 ,\n       1.186477  , 3.5009465 , 1.5893373 , 2.0874112 , 0.609076  ,\n       0.8767379 , 0.46969908, 0.39046878, 1.7755748 , 1.0801693 ,\n       1.6740756 , 0.6364358 , 1.2792948 , 6.878815  , 0.5609837 ,\n       0.6224723 , 3.3456028 , 3.7511055 , 2.8830326 , 2.0223958 ,\n       2.2166443 , 1.790338  , 3.510706  , 1.5492185 , 0.37559396,\n       3.4398756 , 1.8429571 , 3.5136185 , 1.8530477 , 2.0842972 ,\n       2.0739944 , 0.52779543, 1.1062891 , 0.88381904, 1.871858  ,\n       3.154615  , 1.0335704 , 0.48410177, 0.574937  , 1.2008716 ,\n       1.2551936 , 1.309692  , 2.1514757 , 1.7297982 , 1.9524814 ,\n       2.0271084 , 1.4710511 , 0.99491006, 0.5781651 , 2.2892811 ,\n       2.0240502 , 0.39436322, 1.0721147 , 0.43047404, 1.4993033 ,\n       2.6786468 , 0.80048555, 0.62411344, 2.0364773 , 3.0928125 ,\n       1.7218547 , 1.3251877 , 2.109719  , 1.7959052 , 1.9328523 ,\n       1.3739302 , 0.8511676 , 1.4279174 , 1.5229388 , 0.35574773,\n       2.4671147 , 4.422019  , 1.1403834 , 1.7154449 , 1.6801264 ,\n       2.0450542 , 0.9465914 , 1.0075006 , 0.9757275 , 1.3984131 ,\n       2.0132914 , 0.6631341 , 4.2518764 , 0.5079689 , 1.274905  ,\n       1.4539508 , 1.71185   , 0.2967858 , 1.665875  , 0.44874436,\n       0.53122216, 1.8966603 , 1.6310322 , 2.1506674 , 3.8808713 ,\n       0.35948673, 2.0762103 , 1.3816818 , 0.33352643, 1.0317024 ,\n       3.2051365 , 1.4881297 , 2.0626614 , 1.9681727 , 1.7148397 ,\n       0.7811486 , 0.5959722 , 3.9509997 , 1.0954106 , 2.1002245 ,\n       0.9740442 , 0.62698054, 1.6886972 , 1.6553904 , 1.669834  ,\n       0.73685694, 1.5079838 , 2.0114527 , 0.7965228 , 2.8983462 ,\n       2.0881627 , 2.096084  , 1.9354583 , 1.5999545 , 1.2635913 ,\n       1.6995915 , 0.76896447, 2.133092  , 0.53628075, 0.32303035,\n       2.1203187 , 0.79863805, 1.6842881 , 1.6706628 , 1.6953374 ,\n       0.6136588 , 0.5989544 , 1.3435403 , 0.38277063, 1.7931551 ,\n       0.3753876 , 0.5054663 , 0.79168576, 1.0519623 , 1.6409277 ,\n       2.0864563 , 1.3792464 , 0.6706095 , 0.7262162 , 1.979559  ,\n       0.31170422, 0.60782313, 1.0491468 , 2.0242898 , 0.28857803,\n       0.60520864, 1.0712144 , 1.7548578 , 1.1561104 , 1.8513278 ,\n       1.5466387 , 2.0905914 , 1.4313045 , 1.413958  , 1.4712379 ,\n       1.9882215 , 1.8113924 , 2.1033256 , 2.0967498 , 2.0242453 ,\n       2.086597  , 2.08727   , 2.0966399 , 2.1026106 , 2.0685809 ],\n      dtype=float32)"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_users_data = extract_users_data(train_sessions, users, products)\n",
    "x, cat_x = prepare_data_for_predict(extracted_users_data)\n",
    "x = torch.from_numpy(x.values).float()\n",
    "cat_x = torch.from_numpy(cat_x.values).float()\n",
    "some_net.eval()\n",
    "out = some_net(x, cat_x).squeeze()\n",
    "out = out.detach().numpy()\n",
    "out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "data": {
      "text/plain": "{139: 2.5522861,\n 242: 6.670182,\n 108: 1.1285622,\n 143: 0.99783105,\n 140: 4.2777514,\n 202: 2.145917,\n 270: 4.229818,\n 212: 2.3139265,\n 189: 0.45462483,\n 205: 1.608789,\n 255: 2.7811337,\n 215: 0.5814664,\n 273: 0.43894744,\n 281: 2.1252663,\n 227: 3.6308777,\n 222: 1.0397326,\n 235: 1.3970548,\n 171: 2.6281579,\n 188: 0.41820455,\n 135: 2.0558407,\n 252: 1.8769951,\n 238: 2.0723605,\n 145: 1.0095036,\n 286: 1.3822054,\n 157: 0.9210697,\n 265: 1.186477,\n 127: 3.5009465,\n 159: 1.5893373,\n 119: 2.0874112,\n 274: 0.609076,\n 160: 0.8767379,\n 105: 0.46969908,\n 261: 0.39046878,\n 185: 1.7755748,\n 183: 1.0801693,\n 192: 1.6740756,\n 186: 0.6364358,\n 102: 1.2792948,\n 124: 6.878815,\n 282: 0.5609837,\n 210: 0.6224723,\n 125: 3.3456028,\n 272: 3.7511055,\n 104: 2.8830326,\n 115: 2.0223958,\n 136: 2.2166443,\n 285: 1.790338,\n 133: 3.510706,\n 187: 1.5492185,\n 175: 0.37559396,\n 161: 3.4398756,\n 167: 1.8429571,\n 213: 3.5136185,\n 292: 1.8530477,\n 256: 2.0842972,\n 278: 2.0739944,\n 263: 0.52779543,\n 148: 1.1062891,\n 200: 0.88381904,\n 280: 1.871858,\n 287: 3.154615,\n 208: 1.0335704,\n 149: 0.48410177,\n 233: 0.574937,\n 236: 1.2008716,\n 246: 1.2551936,\n 193: 1.309692,\n 147: 2.1514757,\n 158: 1.7297982,\n 251: 1.9524814,\n 277: 2.0271084,\n 182: 1.4710511,\n 151: 0.99491006,\n 253: 0.5781651,\n 295: 2.2892811,\n 262: 2.0240502,\n 178: 0.39436322,\n 156: 1.0721147,\n 110: 0.43047404,\n 120: 1.4993033,\n 118: 2.6786468,\n 204: 0.80048555,\n 300: 0.62411344,\n 128: 2.0364773,\n 260: 3.0928125,\n 141: 1.7218547,\n 190: 1.3251877,\n 114: 2.109719,\n 240: 1.7959052,\n 130: 1.9328523,\n 195: 1.3739302,\n 288: 0.8511676,\n 177: 1.4279174,\n 122: 1.5229388,\n 173: 0.35574773,\n 293: 2.4671147,\n 121: 4.422019,\n 279: 1.1403834,\n 131: 1.7154449,\n 257: 1.6801264,\n 113: 2.0450542,\n 150: 0.9465914,\n 248: 1.0075006,\n 245: 0.9757275,\n 224: 1.3984131,\n 228: 2.0132914,\n 172: 0.6631341,\n 165: 4.2518764,\n 126: 0.5079689,\n 194: 1.274905,\n 164: 1.4539508,\n 268: 1.71185,\n 176: 0.2967858,\n 243: 1.665875,\n 214: 0.44874436,\n 232: 0.53122216,\n 267: 1.8966603,\n 117: 1.6310322,\n 111: 2.1506674,\n 109: 3.8808713,\n 271: 0.35948673,\n 107: 2.0762103,\n 247: 1.3816818,\n 299: 0.33352643,\n 241: 1.0317024,\n 207: 3.2051365,\n 197: 1.4881297,\n 276: 2.0626614,\n 155: 1.9681727,\n 250: 1.7148397,\n 211: 0.7811486,\n 170: 0.5959722,\n 275: 3.9509997,\n 142: 1.0954106,\n 134: 2.1002245,\n 234: 0.9740442,\n 138: 0.62698054,\n 191: 1.6886972,\n 103: 1.6553904,\n 239: 1.669834,\n 226: 0.73685694,\n 198: 1.5079838,\n 266: 2.0114527,\n 154: 0.7965228,\n 223: 2.8983462,\n 237: 2.0881627,\n 123: 2.096084,\n 137: 1.9354583,\n 231: 1.5999545,\n 163: 1.2635913,\n 166: 1.6995915,\n 259: 0.76896447,\n 291: 2.133092,\n 301: 0.53628075,\n 181: 0.32303035,\n 153: 2.1203187,\n 129: 0.79863805,\n 218: 1.6842881,\n 146: 1.6706628,\n 174: 1.6953374,\n 180: 0.6136588,\n 297: 0.5989544,\n 184: 1.3435403,\n 106: 0.38277063,\n 254: 1.7931551,\n 199: 0.3753876,\n 179: 0.5054663,\n 203: 0.79168576,\n 220: 1.0519623,\n 216: 1.6409277,\n 201: 2.0864563,\n 284: 1.3792464,\n 116: 0.6706095,\n 168: 0.7262162,\n 209: 1.979559,\n 298: 0.31170422,\n 112: 0.60782313,\n 219: 1.0491468,\n 196: 2.0242898,\n 290: 0.28857803,\n 249: 0.60520864,\n 294: 1.0712144,\n 296: 1.7548578,\n 230: 1.1561104,\n 258: 1.8513278,\n 217: 1.5466387,\n 244: 2.0905914,\n 132: 1.4313045,\n 144: 1.413958,\n 221: 1.4712379,\n 264: 1.9882215,\n 206: 1.8113924,\n 283: 2.1033256,\n 269: 2.0967498,\n 169: 2.0242453,\n 289: 2.086597,\n 225: 2.08727,\n 152: 2.0966399,\n 162: 2.1026106,\n 229: 2.0685809}"
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out_array = {int(user_id): out[i] > 0.3 for i, user_id in enumerate(extracted_users_data[\"user_id\"].to_list())} binarization\n",
    "out_array = {int(user_id): out[i] for i, user_id in enumerate(extracted_users_data[\"user_id\"].to_list())}\n",
    "out_array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.02000e+02, 0.00000e+00],\n       [1.03000e+02, 5.54770e+00],\n       [1.04000e+02, 2.33201e+01],\n       [1.05000e+02, 0.00000e+00],\n       [1.06000e+02, 0.00000e+00],\n       [1.07000e+02, 7.89000e-01],\n       [1.08000e+02, 0.00000e+00],\n       [1.09000e+02, 1.19980e+00],\n       [1.10000e+02, 1.07900e+01],\n       [1.11000e+02, 6.11800e-01],\n       [1.12000e+02, 3.07320e+00],\n       [1.13000e+02, 0.00000e+00],\n       [1.14000e+02, 0.00000e+00],\n       [1.15000e+02, 0.00000e+00],\n       [1.16000e+02, 9.39440e+00],\n       [1.17000e+02, 0.00000e+00],\n       [1.18000e+02, 0.00000e+00],\n       [1.19000e+02, 0.00000e+00],\n       [1.20000e+02, 0.00000e+00],\n       [1.21000e+02, 1.83196e+01],\n       [1.22000e+02, 2.42602e+01],\n       [1.23000e+02, 2.19990e+01],\n       [1.24000e+02, 2.87002e+01],\n       [1.25000e+02, 1.79000e-01],\n       [1.26000e+02, 0.00000e+00],\n       [1.27000e+02, 2.41701e+01],\n       [1.28000e+02, 9.99900e-01],\n       [1.29000e+02, 2.19990e+01],\n       [1.30000e+02, 2.21200e+01],\n       [1.31000e+02, 0.00000e+00],\n       [1.32000e+02, 5.33790e+01],\n       [1.33000e+02, 2.38182e+01],\n       [1.34000e+02, 7.89000e-01],\n       [1.35000e+02, 6.90000e-01],\n       [1.36000e+02, 1.09000e+00],\n       [1.37000e+02, 3.21530e+01],\n       [1.38000e+02, 2.19990e+01],\n       [1.39000e+02, 0.00000e+00],\n       [1.40000e+02, 1.07900e+01],\n       [1.41000e+02, 0.00000e+00],\n       [1.42000e+02, 0.00000e+00],\n       [1.43000e+02, 5.89700e-01],\n       [1.44000e+02, 2.31702e+01],\n       [1.45000e+02, 5.22020e+00],\n       [1.46000e+02, 0.00000e+00],\n       [1.47000e+02, 2.19990e+01],\n       [1.48000e+02, 1.99900e-01],\n       [1.49000e+02, 0.00000e+00],\n       [1.50000e+02, 0.00000e+00],\n       [1.51000e+02, 7.89000e-01],\n       [1.52000e+02, 0.00000e+00],\n       [1.53000e+02, 5.97990e+00],\n       [1.54000e+02, 0.00000e+00],\n       [1.55000e+02, 6.11970e+00],\n       [1.56000e+02, 0.00000e+00],\n       [1.57000e+02, 0.00000e+00],\n       [1.58000e+02, 2.04850e+01],\n       [1.59000e+02, 2.04850e+01],\n       [1.60000e+02, 5.53000e+00],\n       [1.61000e+02, 2.33989e+01],\n       [1.62000e+02, 0.00000e+00],\n       [1.63000e+02, 0.00000e+00],\n       [1.64000e+02, 5.18497e+01],\n       [1.65000e+02, 1.49900e-01],\n       [1.66000e+02, 0.00000e+00],\n       [1.67000e+02, 1.07900e+01],\n       [1.68000e+02, 1.49900e-01],\n       [1.69000e+02, 0.00000e+00],\n       [1.70000e+02, 1.70180e+00],\n       [1.71000e+02, 1.46270e+00],\n       [1.72000e+02, 7.89000e-01],\n       [1.73000e+02, 7.76902e+01],\n       [1.74000e+02, 0.00000e+00],\n       [1.75000e+02, 1.49900e-01],\n       [1.76000e+02, 2.36997e+01],\n       [1.77000e+02, 7.43180e+00],\n       [1.78000e+02, 2.18890e+00],\n       [1.79000e+02, 0.00000e+00],\n       [1.80000e+02, 5.71000e+00],\n       [1.81000e+02, 1.17899e+01],\n       [1.82000e+02, 1.09000e+00],\n       [1.83000e+02, 0.00000e+00],\n       [1.84000e+02, 2.99000e+00],\n       [1.85000e+02, 6.97970e+00],\n       [1.86000e+02, 0.00000e+00],\n       [1.87000e+02, 0.00000e+00],\n       [1.88000e+02, 0.00000e+00],\n       [1.89000e+02, 1.94597e+01],\n       [1.90000e+02, 0.00000e+00],\n       [1.91000e+02, 0.00000e+00],\n       [1.92000e+02, 0.00000e+00],\n       [1.93000e+02, 0.00000e+00],\n       [1.94000e+02, 0.00000e+00],\n       [1.95000e+02, 2.52900e+00],\n       [1.96000e+02, 5.99900e-01],\n       [1.97000e+02, 0.00000e+00],\n       [1.98000e+02, 0.00000e+00],\n       [1.99000e+02, 3.57970e+00],\n       [2.00000e+02, 2.94992e+01],\n       [2.01000e+02, 2.31702e+01],\n       [2.02000e+02, 6.79790e+01],\n       [2.03000e+02, 5.99000e-01],\n       [2.04000e+02, 1.93000e+00],\n       [2.05000e+02, 0.00000e+00],\n       [2.06000e+02, 0.00000e+00],\n       [2.07000e+02, 1.68790e+00],\n       [2.08000e+02, 0.00000e+00],\n       [2.09000e+02, 1.49900e-01],\n       [2.10000e+02, 1.19790e+00],\n       [2.11000e+02, 3.87295e+01],\n       [2.12000e+02, 0.00000e+00],\n       [2.13000e+02, 1.39980e+00],\n       [2.14000e+02, 3.02900e+01],\n       [2.15000e+02, 0.00000e+00],\n       [2.16000e+02, 0.00000e+00],\n       [2.17000e+02, 1.99980e+00],\n       [2.18000e+02, 0.00000e+00],\n       [2.19000e+02, 0.00000e+00],\n       [2.20000e+02, 0.00000e+00],\n       [2.21000e+02, 7.99000e-01],\n       [2.22000e+02, 1.58990e+00],\n       [2.23000e+02, 3.27470e+00],\n       [2.24000e+02, 2.07330e+00],\n       [2.25000e+02, 0.00000e+00],\n       [2.26000e+02, 0.00000e+00],\n       [2.27000e+02, 5.53000e+00],\n       [2.28000e+02, 3.49000e+00],\n       [2.29000e+02, 0.00000e+00],\n       [2.30000e+02, 0.00000e+00],\n       [2.31000e+02, 0.00000e+00],\n       [2.32000e+02, 3.29900e-01],\n       [2.33000e+02, 0.00000e+00],\n       [2.34000e+02, 0.00000e+00],\n       [2.35000e+02, 1.72490e+00],\n       [2.36000e+02, 0.00000e+00],\n       [2.37000e+02, 9.99900e-01],\n       [2.38000e+02, 1.93000e+00],\n       [2.39000e+02, 0.00000e+00],\n       [2.40000e+02, 0.00000e+00],\n       [2.41000e+02, 1.29990e+00],\n       [2.42000e+02, 1.58960e+00],\n       [2.43000e+02, 4.99900e-01],\n       [2.44000e+02, 0.00000e+00],\n       [2.45000e+02, 0.00000e+00],\n       [2.46000e+02, 7.89000e-01],\n       [2.47000e+02, 9.32000e+00],\n       [2.48000e+02, 1.89000e+00],\n       [2.49000e+02, 0.00000e+00],\n       [2.50000e+02, 1.89387e+01],\n       [2.51000e+02, 1.63860e+00],\n       [2.52000e+02, 0.00000e+00],\n       [2.53000e+02, 5.53000e+00],\n       [2.54000e+02, 8.78900e-01],\n       [2.55000e+02, 0.00000e+00],\n       [2.56000e+02, 0.00000e+00],\n       [2.57000e+02, 0.00000e+00],\n       [2.58000e+02, 9.99900e-01],\n       [2.59000e+02, 0.00000e+00],\n       [2.60000e+02, 7.60330e+00],\n       [2.61000e+02, 0.00000e+00],\n       [2.62000e+02, 0.00000e+00],\n       [2.63000e+02, 0.00000e+00],\n       [2.64000e+02, 0.00000e+00],\n       [2.65000e+02, 3.29900e-01],\n       [2.66000e+02, 5.30190e+01],\n       [2.67000e+02, 6.48000e-01],\n       [2.68000e+02, 0.00000e+00],\n       [2.69000e+02, 0.00000e+00],\n       [2.70000e+02, 6.11970e+00],\n       [2.71000e+02, 0.00000e+00],\n       [2.72000e+02, 8.32280e+00],\n       [2.73000e+02, 5.71501e+01],\n       [2.74000e+02, 2.31702e+01],\n       [2.75000e+02, 7.99000e-01],\n       [2.76000e+02, 2.07330e+00],\n       [2.77000e+02, 4.80690e+01],\n       [2.78000e+02, 0.00000e+00],\n       [2.79000e+02, 0.00000e+00],\n       [2.80000e+02, 2.83890e+01],\n       [2.81000e+02, 0.00000e+00],\n       [2.82000e+02, 0.00000e+00],\n       [2.83000e+02, 0.00000e+00],\n       [2.84000e+02, 0.00000e+00],\n       [2.85000e+02, 0.00000e+00],\n       [2.86000e+02, 7.89000e-01],\n       [2.87000e+02, 4.76679e+01],\n       [2.88000e+02, 7.39000e+00],\n       [2.89000e+02, 0.00000e+00],\n       [2.90000e+02, 9.28900e-01],\n       [2.91000e+02, 0.00000e+00],\n       [2.92000e+02, 5.89700e-01],\n       [2.93000e+02, 0.00000e+00],\n       [2.94000e+02, 0.00000e+00],\n       [2.95000e+02, 1.68890e+00],\n       [2.96000e+02, 0.00000e+00],\n       [2.97000e+02, 1.09000e+00],\n       [2.98000e+02, 2.39900e+01],\n       [2.99000e+02, 0.00000e+00],\n       [3.00000e+02, 0.00000e+00],\n       [3.01000e+02, 0.00000e+00]])"
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n"
     ]
    }
   ],
   "source": [
    "numerator = 0\n",
    "for row in targets.values[:]:\n",
    "    if row[1] == out_array[row[0]]:\n",
    "        print(row[0])\n",
    "        numerator += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0927],\n        [0.0727],\n        [0.1011],\n        [0.0847],\n        [0.0827],\n        [0.0970],\n        [0.0828],\n        [0.0978],\n        [0.0843],\n        [0.0618],\n        [0.0891],\n        [0.0878],\n        [0.0957],\n        [0.0425],\n        [0.0880],\n        [0.1005],\n        [0.0806],\n        [0.0935],\n        [0.0873],\n        [0.0487],\n        [0.0544],\n        [0.0928],\n        [0.0856],\n        [0.0696],\n        [0.0872],\n        [0.0833],\n        [0.0868],\n        [0.0696],\n        [0.0496],\n        [0.0898],\n        [0.1002],\n        [0.0917],\n        [0.0890],\n        [0.1001],\n        [0.0772],\n        [0.0677],\n        [0.0891],\n        [0.0797],\n        [0.0741],\n        [0.0906],\n        [0.0902],\n        [0.0884],\n        [0.0882],\n        [0.0929],\n        [0.0480],\n        [0.0970],\n        [0.0597],\n        [0.0915],\n        [0.0713],\n        [0.0898],\n        [0.0876],\n        [0.0526],\n        [0.0895],\n        [0.0939],\n        [0.0432],\n        [0.0440],\n        [0.0988],\n        [0.0832],\n        [0.0880],\n        [0.0983],\n        [0.0906],\n        [0.0848],\n        [0.0989],\n        [0.0964],\n        [0.0748],\n        [0.1001],\n        [0.0744],\n        [0.0429],\n        [0.0996],\n        [0.0975],\n        [0.0955],\n        [0.1003],\n        [0.0815],\n        [0.0912],\n        [0.0928],\n        [0.0483],\n        [0.0929],\n        [0.0862],\n        [0.0927],\n        [0.0984],\n        [0.0933],\n        [0.1001],\n        [0.0984],\n        [0.0950],\n        [0.0933],\n        [0.0570],\n        [0.0718],\n        [0.0446],\n        [0.0518],\n        [0.0982],\n        [0.1003],\n        [0.0874],\n        [0.1016],\n        [0.0722],\n        [0.0892],\n        [0.0956],\n        [0.0864],\n        [0.1007],\n        [0.0627],\n        [0.0572],\n        [0.0974],\n        [0.1002],\n        [0.0814],\n        [0.0823],\n        [0.0771],\n        [0.0508],\n        [0.0997],\n        [0.0880],\n        [0.0920],\n        [0.0806],\n        [0.0667],\n        [0.0540],\n        [0.0918],\n        [0.0643],\n        [0.0919],\n        [0.0908],\n        [0.0553],\n        [0.0624],\n        [0.0976],\n        [0.0903],\n        [0.0853],\n        [0.0537],\n        [0.0752],\n        [0.0902],\n        [0.0861],\n        [0.0931],\n        [0.1001],\n        [0.0995],\n        [0.0988],\n        [0.0990],\n        [0.0891],\n        [0.0911],\n        [0.0894],\n        [0.0803],\n        [0.0428],\n        [0.0844],\n        [0.0986],\n        [0.0612],\n        [0.1006],\n        [0.0648],\n        [0.0898],\n        [0.0705],\n        [0.0986],\n        [0.0890],\n        [0.0980],\n        [0.0983],\n        [0.0459],\n        [0.0998],\n        [0.0583],\n        [0.0818],\n        [0.0568],\n        [0.1009],\n        [0.0398],\n        [0.0942],\n        [0.0871],\n        [0.0439],\n        [0.1007],\n        [0.0567],\n        [0.0553],\n        [0.1019],\n        [0.0907],\n        [0.0907],\n        [0.0782],\n        [0.0938],\n        [0.0639],\n        [0.0919],\n        [0.0932],\n        [0.0857],\n        [0.0846],\n        [0.0699],\n        [0.0419],\n        [0.0718],\n        [0.0898],\n        [0.1007],\n        [0.0470],\n        [0.0928],\n        [0.0971],\n        [0.0775],\n        [0.0487],\n        [0.0900],\n        [0.0898],\n        [0.0784],\n        [0.0591],\n        [0.0779],\n        [0.0564],\n        [0.1008],\n        [0.0429],\n        [0.0749],\n        [0.0708],\n        [0.0686],\n        [0.0445],\n        [0.0508],\n        [0.0435],\n        [0.0418],\n        [0.0464],\n        [0.0399],\n        [0.0405],\n        [0.0418],\n        [0.0401],\n        [0.0415]])"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_net(x, cat_x).detach()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "0.6915807560137457"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc(targets.expenses, some_net(x, cat_x).detach())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "torch.save(some_net.state_dict(), \"../models/parameters/dummy_test\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}